# src/r_core/hippocampus.py
"""
üß† Hippocampus Consolidation Module

–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
–ì–∏–ø–ø–æ–∫–∞–º–ø - —ç—Ç–æ –ù–ï —Ö—Ä–∞–Ω–∏–ª–∏—â–µ, –∞ –ü–†–û–¶–ï–°–°–û–†.
–û–Ω —Å–ª—É–∂–∏—Ç –º–æ—Å—Ç–æ–º –º–µ–∂–¥—É episodic_memory (—Å—ã—Ä—ã–µ —ç–ø–∏–∑–æ–¥—ã) –∏ semantic_memory (–∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã).

–¢—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:
1. Deduplicate semantic facts (—Å–ª–∏—è–Ω–∏–µ –¥—É–±–ª–µ–π —Ñ–∞–∫—Ç–æ–≤)
2. Extract facts from episodes (–∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è: —ç–ø–∏–∑–æ–¥ ‚Üí —Å–µ–º–∞–Ω—Ç–∏–∫–∞)
3. Update volitional patterns (–≤—ã—è–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –∏—Ö –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞)

–¢—Ä–∏–≥–≥–µ—Ä: user_profiles.short_term_memory_load >= 20
"""

import asyncio
import json
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Set, Tuple, Union
from sqlalchemy import select, text, delete, and_, update
from sqlalchemy.ext.asyncio import AsyncSession
from src.r_core.infrastructure.db import (
    AsyncSessionLocal,
    SemanticModel,
    EpisodicModel,
    VolitionalModel,
    UserProfileModel
)
from src.r_core.config import settings


@dataclass
class SemanticFact:
    """Semantic memory fact wrapper"""
    id: int
    subject: str
    predicate: str
    object: str
    confidence: float
    embedding: Optional[List[float]]
    created_at: datetime
    sentiment: Optional[Dict[str, Any]] = None


@dataclass
class Episode:
    """Episodic memory episode wrapper"""
    id: int
    raw_text: str
    embedding: List[float]
    emotion_score: float
    tags: List[str]
    created_at: datetime


class Hippocampus:
    """
    üß† Memory Consolidation Engine
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç "–ª–µ–Ω–∏–≤—É—é" –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –ø–∞–º—è—Ç–∏.
    –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–∞–∂–¥—ã–µ N —ç–ø–∏–∑–æ–¥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é N=20).
    """
    
    def __init__(
        self,
        llm_client,  # LLM client –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        embedding_client,  # Embedding client –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        *,
        similarity_threshold: float = 0.85,  # –ø–æ—Ä–æ–≥ –¥–ª—è pgvector –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        max_cluster_size: int = 10,  # –º–∞–∫—Å —Ñ–∞–∫—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        episode_window_days: int = 30,  # –æ–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ø–∏–∑–æ–¥–æ–≤
        min_theme_frequency: int = 3,  # –º–∏–Ω. –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–∫—Ç–∞
    ):
        self.llm = llm_client
        self.embedder = embedding_client
        self.similarity_threshold = similarity_threshold
        self.max_cluster_size = max_cluster_size
        self.episode_window_days = episode_window_days
        self.min_theme_frequency = min_theme_frequency

    def _ensure_list(self, embedding: Any) -> List[float]:
        """Helper to ensure embedding is a python list, not numpy array"""
        if hasattr(embedding, 'tolist'):
            return embedding.tolist()
        if isinstance(embedding, list):
            return embedding
        return []
    
    def _serialize_vector(self, embedding: Any) -> Optional[str]:
        """
        Robustly serialize vector to pgvector string format '[1.0,2.0,3.0]'.
        Returns None if embedding is missing or empty.
        """
        vec_list = self._ensure_list(embedding)
        if not vec_list:
            return None
        # pgvector expects '[1,2,3]' format. json.dumps produces exactly this for lists.
        return json.dumps(vec_list)

    async def consolidate(self, user_id: int) -> Dict[str, Any]:
        """
        üéì –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        
        Returns:
            {
                "status": "ok" | "skip" | "error",
                "tasks_completed": ["deduplicate", "extract", "volitional"],
                "stats": {...}
            }
        """
        print(f"[Hippocampus] Starting consolidation for user {user_id}")
        
        stats = {
            "task_1_deduplicate": {},
            "task_2_extract": {},
            "task_3_volitional": {},
        }
        
        try:
            # Task 1: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è semantic_memory
            stats["task_1_deduplicate"] = await self._deduplicate_semantic_facts(user_id)
            
            # Task 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —ç–ø–∏–∑–æ–¥–æ–≤
            stats["task_2_extract"] = await self._extract_facts_from_episodes(user_id)
            
            # Task 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ volitional_patterns
            stats["task_3_volitional"] = await self._update_volitional_patterns(user_id)
            
            # –°–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞
            await self._reset_consolidation_counter(user_id)
            
            print(f"[Hippocampus] ‚úÖ Consolidation complete: {stats}")
            return {
                "status": "ok",
                "tasks_completed": ["deduplicate", "extract", "volitional"],
                "stats": stats
            }
            
        except Exception as e:
            print(f"[Hippocampus] ‚ùå Consolidation failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "stats": stats
            }
    
    # ========== TASK 1: Deduplicate Semantic Facts ==========
    
    async def _deduplicate_semantic_facts(self, user_id: int) -> Dict[str, Any]:
        """
        Task 1: –°–ª–∏—è–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ semantic_memory
        
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. pgvector –Ω–∞—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∞–∫—Ç–æ–≤ (recall)
        2. LLM –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (precision)
        3. –°–æ–∑–¥–∞—ë—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç, —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏
        """
        async with AsyncSessionLocal() as session:
            # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–∫—Ç—ã —Å embedding
            facts = await self._load_semantic_facts_with_embeddings(session, user_id)
            
            if len(facts) < 3:
                return {"status": "skip", "reason": "too_few_facts", "count": len(facts)}
            
            # 2. –ù–∞–π—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä—ã —á–µ—Ä–µ–∑ pgvector
            clusters = await self._find_similar_fact_clusters(session, user_id, facts)
            
            if not clusters:
                return {"status": "skip", "reason": "no_duplicates", "facts_checked": len(facts)}
            
            # 3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞: LLM merge
            merged_count = 0
            for cluster_ids in clusters:
                cluster_facts = [f for f in facts if f.id in cluster_ids]
                if len(cluster_facts) < 2:
                    continue
                
                # LLM —Å–ª–∏–≤–∞–µ—Ç —Ñ–∞–∫—Ç—ã
                canonical = await self._llm_merge_facts(cluster_facts)
                if not canonical:
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç, —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ
                await self._upsert_canonical_fact(session, user_id, canonical, cluster_ids)
                merged_count += 1
            
            await session.commit()
            
            return {
                "status": "ok",
                "facts_checked": len(facts),
                "clusters_found": len(clusters),
                "clusters_merged": merged_count
            }
    
    async def _load_semantic_facts_with_embeddings(
        self, session: AsyncSession, user_id: int
    ) -> List[SemanticFact]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–∫—Ç—ã —Å embedding (—Ç–æ–ª—å–∫–æ —Ç–µ, –≥–¥–µ embedding IS NOT NULL)"""
        result = await session.execute(
            select(SemanticModel)
            .where(
                and_(
                    SemanticModel.user_id == user_id,
                    SemanticModel.embedding.isnot(None)
                )
            )
            .order_by(SemanticModel.created_at.desc())
            .limit(200)  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        )
        rows = result.scalars().all()
        
        return [
            SemanticFact(
                id=row.id,
                subject=row.subject,
                predicate=row.predicate,
                object=row.object,
                confidence=row.confidence,
                embedding=row.embedding,
                created_at=row.created_at,
                sentiment=row.sentiment
            )
            for row in rows
        ]
    
    async def _find_similar_fact_clusters(
        self, session: AsyncSession, user_id: int, facts: List[SemanticFact]
    ) -> List[Set[int]]:
        """
        –ù–∞–π—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ pgvector cosine similarity.
        
        Returns: List of Sets, –∫–∞–∂–¥—ã–π set = {fact_id1, fact_id2, ...}
        """
        clusters: List[Set[int]] = []
        processed: Set[int] = set()
        
        for fact in facts:
            if fact.id in processed or fact.embedding is None:
                continue
            
            # Use new robust serializer
            emb_str = self._serialize_vector(fact.embedding)
            if not emb_str:
                continue
            
            # –ù–∞–π—Ç–∏ —Å–æ—Å–µ–¥–µ–π —á–µ—Ä–µ–∑ pgvector
            query = text("""
                SELECT id, 1 - (embedding <=> :target_embedding) AS similarity
                FROM semantic_memory
                WHERE user_id = :user_id
                  AND id != :fact_id
                  AND embedding IS NOT NULL
                  AND 1 - (embedding <=> :target_embedding) >= :threshold
                ORDER BY similarity DESC
                LIMIT :max_size
            """)
            
            result = await session.execute(query, {
                "user_id": user_id,
                "fact_id": fact.id,
                "target_embedding": emb_str,
                "threshold": self.similarity_threshold,
                "max_size": self.max_cluster_size
            })
            
            neighbors = result.fetchall()
            
            if neighbors:
                cluster = {fact.id} | {row.id for row in neighbors}
                clusters.append(cluster)
                processed.update(cluster)
        
        return clusters
    
    async def _llm_merge_facts(self, facts: List[SemanticFact]) -> Optional[Dict[str, Any]]:
        """
        LLM —Å–ª–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–æ–≤ –≤ –æ–¥–∏–Ω –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π.
        
        Returns:
            {
                "subject": "...",
                "predicate": "...",
                "object": "...",
                "confidence": 0.0-1.0,
                "sentiment": {...} or None
            }
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        facts_text = "\n".join([
            f"{i+1}. {f.subject} {f.predicate} {f.object} (confidence: {f.confidence:.2f})"
            for i, f in enumerate(facts)
        ])
        
        prompt = f"""
–¢—ã - —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏.
–ó–∞–¥–∞—á–∞: —Å–ª–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∞–∫—Ç–æ–≤ –≤ –æ–¥–∏–Ω –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π.

–§–∞–∫—Ç—ã:
{facts_text}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:
- –°–æ—Ö—Ä–∞–Ω–∏ –æ–±—â–∏–π —Å–º—ã—Å–ª, —É–±–µ—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã.
- confidence = —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ (—Å —É—á—ë—Ç–æ–º —Å—Ç–∞—Ä—ã—Ö confidence).
- –ï—Å–ª–∏ —Ñ–∞–∫—Ç—ã –ù–ï —è–≤–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏ (—Ä–∞–∑–Ω—ã–π —Å–º—ã—Å–ª), –≤–µ—Ä–Ω–∏ null.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (JSON):
{{
    "is_duplicate": true/false,
    "canonical": {{
        "subject": "...",
        "predicate": "...",
        "object": "...",
        "confidence": 0.85
    }}
}}
"""
        
        try:
            # –í—ã–∑—ã–≤–∞–µ–º LLM (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ llm.complete –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É)
            response = await self.llm.complete(prompt)
            data = json.loads(response)
            
            if not data.get("is_duplicate", False):
                return None
            
            canonical = data.get("canonical")
            if not canonical or not all(k in canonical for k in ["subject", "predicate", "object"]):
                return None
            
            return canonical
            
        except Exception as e:
            print(f"[Hippocampus] LLM merge failed: {e}")
            return None
    
    async def _upsert_canonical_fact(
        self,
        session: AsyncSession,
        user_id: int,
        canonical: Dict[str, Any],
        old_fact_ids: Set[int]
    ):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç, —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ"""
        # –°–æ–∑–¥–∞—ë–º embedding –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ñ–∞–∫—Ç–∞
        fact_text = f"{canonical['subject']} {canonical['predicate']} {canonical['object']}"
        embedding = await self.embedder.embed(fact_text)
        
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Ñ–∞–∫—Ç
        new_fact = SemanticModel(
            user_id=user_id,
            subject=canonical["subject"],
            predicate=canonical["predicate"],
            object=canonical["object"],
            confidence=canonical.get("confidence", 0.8),
            embedding=embedding,
            sentiment=canonical.get("sentiment"),
            source_message_id="hippocampus_consolidation"
        )
        session.add(new_fact)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ
        await session.execute(
            delete(SemanticModel).where(SemanticModel.id.in_(old_fact_ids))
        )
    
    # ========== TASK 2: Extract Facts from Episodes ==========
    
    async def _extract_facts_from_episodes(self, user_id: int) -> Dict[str, Any]:
        """
        Task 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏–∑ episodic_memory.
        
        –ü—Ä–∏–º–µ—Ä: 5 —ç–ø–∏–∑–æ–¥–æ–≤ —É–ø–æ–º–∏–Ω–∞—é—Ç "Python" ‚Üí —Ñ–∞–∫—Ç: "User INTERESTED_IN Python"
        """
        async with AsyncSessionLocal() as session:
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —ç–ø–∏–∑–æ–¥–æ–≤
            cutoff_date = datetime.utcnow() - timedelta(days=self.episode_window_days)
            result = await session.execute(
                select(EpisodicModel)
                .where(
                    and_(
                        EpisodicModel.user_id == user_id,
                        EpisodicModel.created_at >= cutoff_date
                    )
                )
                .order_by(EpisodicModel.created_at.desc())
                .limit(100)
            )
            episodes = result.scalars().all()
            
            if len(episodes) < self.min_theme_frequency:
                return {"status": "skip", "reason": "too_few_episodes", "count": len(episodes)}
            
            # LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º—ã
            themes = await self._llm_extract_themes(episodes)
            
            if not themes:
                return {"status": "skip", "reason": "no_themes_found", "episodes_checked": len(episodes)}
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã
            for theme in themes:
                fact_text = f"{theme['subject']} {theme['predicate']} {theme['object']}"
                embedding = await self.embedder.embed(fact_text)
                
                new_fact = SemanticModel(
                    user_id=user_id,
                    subject=theme["subject"],
                    predicate=theme["predicate"],
                    object=theme["object"],
                    confidence=theme.get("confidence", 0.7),
                    embedding=embedding,
                    source_message_id="hippocampus_episode_extraction"
                )
                session.add(new_fact)
            
            await session.commit()
            
            return {
                "status": "ok",
                "episodes_analyzed": len(episodes),
                "themes_extracted": len(themes)
            }
    
    async def _llm_extract_themes(self, episodes: List[EpisodicModel]) -> List[Dict[str, Any]]:
        """
        LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–ø–∏–∑–æ–¥—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ–º—ã/–∏–Ω—Ç–µ—Ä–µ—Å—ã.
        """
        episodes_text = "\n".join([
            f"{i+1}. {ep.raw_text[:200]}"
            for i, ep in enumerate(episodes[:50])  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        ])
        
        prompt = f"""
–¢—ã - —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏.
–ó–∞–¥–∞—á–∞: –Ω–∞–π—Ç–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ–º—ã/–∏–Ω—Ç–µ—Ä–µ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–≠–ø–∏–∑–æ–¥—ã:
{episodes_text}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:
- –ù–∞–π–¥–∏ —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è >= {self.min_theme_frequency} —Ä–∞–∑.
- –°–æ–∑–¥–∞–π —Ñ–∞–∫—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Subject-Predicate-Object.
- confidence = frequency / total_episodes.

–ü—Ä–∏–º–µ—Ä:
{{
    "themes": [
        {{\"subject\": \"User\", \"predicate\": \"INTERESTED_IN\", \"object\": \"Python\", \"confidence\": 0.75}},
        {{\"subject\": \"User\", \"predicate\": \"DISLIKES\", \"object\": \"—Ö–æ–ª–æ–¥–Ω–∞—è –ø–æ–≥–æ–¥–∞\", \"confidence\": 0.6}}
    ]
}}

–í–µ—Ä–Ω–∏ JSON:
"""
        
        try:
            response = await self.llm.complete(prompt)
            data = json.loads(response)
            return data.get("themes", [])
        except Exception as e:
            print(f"[Hippocampus] Theme extraction failed: {e}")
            return []
    
    # ========== TASK 3: Update Volitional Patterns ==========
    
    async def _update_volitional_patterns(self, user_id: int) -> Dict[str, Any]:
        """
        Task 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ volitional_memory (–ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è).
        
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:
        - –í—Ä–µ–º—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (night_owl / morning_person)
        - –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å (high / low energy)
        
        ‚ú® NEW (Update): Reinforcement Learning of Volition
        –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è, –º—ã —Ç–µ–ø–µ—Ä—å:
        1. –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω.
        2. –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º learned_delta (–æ–±—É—á–µ–Ω–∏–µ) –∏ –æ–±–Ω–æ–≤–ª—è–µ–º last_activated_at.
        3. –ï—Å–ª–∏ –Ω–µ—Ç: –°–æ–∑–¥–∞–µ–º —Å –±–∞–∑–æ–≤—ã–º intensity.
        """
        async with AsyncSessionLocal() as session:
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å —ç–ø–∏–∑–æ–¥—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
            cutoff_date = datetime.utcnow() - timedelta(days=self.episode_window_days)
            result = await session.execute(
                select(EpisodicModel)
                .where(
                    and_(
                        EpisodicModel.user_id == user_id,
                        EpisodicModel.created_at >= cutoff_date
                    )
                )
                .order_by(EpisodicModel.created_at.desc())
            )
            episodes = result.scalars().all()
            
            if len(episodes) < 5:
                return {"status": "skip", "reason": "too_few_episodes", "count": len(episodes)}
            
            patterns_updated = 0
            
            # --- Analysis 1: Night Owl ---
            timestamps = [ep.created_at for ep in episodes]
            is_night_owl = self._detect_night_owl(timestamps)
            
            if is_night_owl:
                await self._reinforce_or_create_pattern(
                    session, user_id,
                    trigger="time > 22:00",
                    impulse="initiate_dialogue",
                    defaults={
                        "goal": "social_connection",
                        "resolution_strategy": "wait_for_user",
                        "action_taken": "detected_night_owl_pattern",
                        "intensity": 0.4  # –ë–∞–∑–æ–≤–∞—è —Å–∏–ª–∞ —Ö—Ä–æ–Ω–æ—Ç–∏–ø–∞
                    }
                )
                patterns_updated += 1
            
            # --- Analysis 2: High Energy ---
            avg_emotion = sum(ep.emotion_score for ep in episodes) / len(episodes)
            energy_level = min(1.0, max(0.0, (avg_emotion + 1.0) / 2.0))  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è -1..1 ‚Üí 0..1
            
            if energy_level > 0.7:
                await self._reinforce_or_create_pattern(
                    session, user_id,
                    trigger="user_message_received",
                    impulse="high_energy",
                    defaults={
                        "goal": "maintain_engagement",
                        "resolution_strategy": "mirror_energy",
                        "action_taken": f"detected_high_energy (avg={energy_level:.2f})",
                        "intensity": 0.7  # –í—ã—Å–æ–∫–∞—è –±–∞–∑–æ–≤–∞—è —Å–∏–ª–∞
                    }
                )
                patterns_updated += 1
            
            await session.commit()
            
            return {
                "status": "ok",
                "episodes_analyzed": len(episodes),
                "patterns_updated": patterns_updated
            }
            
    async def _reinforce_or_create_pattern(
        self, 
        session: AsyncSession, 
        user_id: int, 
        trigger: str, 
        impulse: str, 
        defaults: Dict[str, Any]
    ):
        """
        Helper: –ò—â–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω. –ï—Å–ª–∏ –µ—Å—Ç—å ‚Äî —É—Å–∏–ª–∏–≤–∞–µ—Ç (Reinforcement). –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞–µ—Ç.
        """
        # 1. Try to find existing pattern
        result = await session.execute(
            select(VolitionalModel).where(
                and_(
                    VolitionalModel.user_id == user_id,
                    VolitionalModel.trigger == trigger,
                    VolitionalModel.impulse == impulse
                )
            )
        )
        pattern = result.scalar_one_or_none()
        
        if pattern:
            # ‚ú® REINFORCEMENT: –£—Å–∏–ª–∏–≤–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
            # learned_delta —Ä–∞—Å—Ç–µ—Ç, –Ω–æ –Ω–µ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ (clamp -1.0 to +1.0)
            new_delta = pattern.learned_delta + pattern.reinforcement_rate
            pattern.learned_delta = max(-1.0, min(1.0, new_delta))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–≤–∞–∂–Ω–æ –¥–ª—è decay)
            pattern.last_activated_at = datetime.utcnow()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —ç–Ω–µ—Ä–≥–∏–∏)
            if "action_taken" in defaults:
                pattern.action_taken = defaults["action_taken"]
                
        else:
            # ‚ú® CREATE: –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            new_pattern = VolitionalModel(
                user_id=user_id,
                trigger=trigger,
                impulse=impulse,
                goal=defaults.get("goal"),
                resolution_strategy=defaults.get("resolution_strategy"),
                action_taken=defaults.get("action_taken"),
                intensity=defaults.get("intensity", 0.5),
                learned_delta=0.0,
                last_activated_at=datetime.utcnow()
            )
            session.add(new_pattern)

    def _detect_night_owl(self, timestamps: List[datetime]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–æ—á–Ω–æ–π —Å–æ–≤–æ–π (>50% –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ 22:00-06:00)"""
        night_count = sum(1 for ts in timestamps if ts.hour >= 22 or ts.hour <= 6)
        return night_count / len(timestamps) > 0.5
    
    # ========== Utility ==========
    
    async def _reset_consolidation_counter(self, user_id: int):
        """–°–±—Ä–æ—Å–∏—Ç—å short_term_memory_load = 0 –∏ –æ–±–Ω–æ–≤–∏—Ç—å last_consolidation_at"""
        async with AsyncSessionLocal() as session:
            await session.execute(
                text("""
                    UPDATE user_profiles
                    SET short_term_memory_load = 0,
                        last_consolidation_at = NOW()
                    WHERE user_id = :user_id
                """),
                {"user_id": user_id}
            )
            await session.commit()
            
    # ========== ‚ú® NEW: Predictive Processing Methods (Phase 2.2) ==========

    async def save_prediction(self, 
                            user_id: int, 
                            session_id: str, 
                            bot_message: str, 
                            predicted_reaction: str,
                            predicted_embedding: Optional[List[float]] = None):
        """
        Save the bot's prediction about the user's NEXT move.
        Uses raw SQL to avoid needing a new model definition right now.
        """
        # FIX: Ensure python list for json serialization
        emb_str = self._serialize_vector(predicted_embedding)
        
        async with AsyncSessionLocal() as session:
            await session.execute(
                text("""
                    INSERT INTO prediction_history 
                    (user_id, session_id, bot_message, predicted_reaction, predicted_embedding)
                    VALUES (:user_id, :session_id, :bot_message, :predicted_reaction, :predicted_embedding::vector)
                """),
                {
                    "user_id": user_id,
                    "session_id": session_id,
                    "bot_message": bot_message,
                    "predicted_reaction": predicted_reaction,
                    "predicted_embedding": emb_str
                }
            )
            await session.commit()

    async def get_last_prediction(self, session_id: str) -> Optional[Dict]:
        """
        Get the most recent UNVERIFIED prediction for this session.
        """
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                text("""
                    SELECT * FROM prediction_history 
                    WHERE session_id = :session_id 
                    ORDER BY created_at DESC 
                    LIMIT 1
                """),
                {"session_id": session_id}
            )
            
            # SQLAlchemy returns Row objects, convert to dict
            row = result.fetchone()
            if not row:
                return None
                
            # Accessing row columns by index or name depends on driver, 
            # assuming name access or mapping.
            # Convert row to dict safely
            try:
                data = row._mapping
            except AttributeError:
                # Fallback for older alchemy
                keys = result.keys()
                data = dict(zip(keys, row))
            
            # If actual_message is present, it's already verified
            if data["actual_message"] is not None:
                return None
                
            return dict(data)

    async def verify_prediction(self, 
                              prediction_id: int, 
                              actual_message: str, 
                              prediction_error: float,
                              actual_embedding: Optional[List[float]] = None):
        """
        Update the prediction record with the actual outcome and calculated error.
        """
        # FIX: Ensure python list for json serialization
        emb_str = self._serialize_vector(actual_embedding)
        
        async with AsyncSessionLocal() as session:
            await session.execute(
                text("""
                    UPDATE prediction_history
                    SET actual_message = :actual_message,
                        actual_embedding = :actual_embedding::vector,
                        prediction_error = :prediction_error,
                        verified_at = NOW()
                    WHERE id = :prediction_id
                """),
                {
                    "actual_message": actual_message,
                    "actual_embedding": emb_str,
                    "prediction_error": prediction_error,
                    "prediction_id": prediction_id
                }
            )
            await session.commit()
